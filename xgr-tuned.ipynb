{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7141985,"sourceType":"datasetVersion","datasetId":4122233}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-06T23:49:51.310420Z","iopub.execute_input":"2023-12-06T23:49:51.311427Z","iopub.status.idle":"2023-12-06T23:49:51.878811Z","shell.execute_reply.started":"2023-12-06T23:49:51.311375Z","shell.execute_reply":"2023-12-06T23:49:51.877374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\n\n# Read the data\ndata = pd.read_csv('/kaggle/input/datalast/train_test_data.csv')\n\n# Extract 'Category of transaction' and 'Type of transaction'\nsplit_columns = data['category_transaction'].str.split('-', n=1, expand=True)\ndata['Category of transaction'] = split_columns[0]\ndata['Type of transaction'] = split_columns[1]\ndata.drop('category_transaction', axis=1, inplace=True)\n\n# Convert 'Date' to datetime\ndata['Date'] = pd.to_datetime(data['Date'], format='%Y-%m-%d %H:%M:%S')\n\n# Create time series features\ndef create_features(data):\n    data = data.copy()\n    data['dayofweek'] = data['Date'].dt.dayofweek\n    data['quarter'] = data['Date'].dt.quarter\n    data['month'] = data['Date'].dt.month\n    data['year'] = data['Date'].dt.year\n    data['dayofyear'] = data['Date'].dt.dayofyear\n    data['dayofmonth'] = data['Date'].dt.day\n    data['weekofyear'] = data['Date'].dt.isocalendar().week\n    return data\n\ndata = create_features(data)\n\n# Encode categorical columns\nle = LabelEncoder()\ndata['Category_encoded'] = le.fit_transform(data['Category of transaction'])\ndata['Type_encoded'] = le.fit_transform(data['Type of transaction'])\ndata['Id_compte_encoded'] = le.fit_transform(data['Id_compte'])\n\n# Drop unnecessary columns\ncolumns_to_drop = ['Id_compte', 'Type of transaction', 'Category of transaction', 'Date']\ndata = data.drop(columns=columns_to_drop)\n\n# Split the data\nX = data.drop(['amount_transaction'], axis=1)\nY = data['amount_transaction']\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=2)\n\n# Feature Scaling\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Hyperparameter Tuning for XGBoost\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [3, 5, 7],\n    'learning_rate': [0.01, 0.1, 0.2],\n    'subsample': [0.8, 1.0],\n    'colsample_bytree': [0.8, 1.0],\n}\n\nxgb_model = XGBRegressor()\ngrid_search = GridSearchCV(xgb_model, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\ngrid_search.fit(X_train_scaled, Y_train)\n\n# Get the best parameters\nbest_params = grid_search.best_params_\nprint(\"Best Hyperparameters:\", best_params)\n\n# Use the best model\nbest_model = grid_search.best_estimator_\n\n# Make predictions on the test set\ny_pred = best_model.predict(X_test_scaled)\n\n# Calculate Mean Squared Error\nmse = mean_squared_error(Y_test, y_pred)\nprint(\"Mean Squared Error on Test Set:\", mse)","metadata":{"execution":{"iopub.status.busy":"2023-12-06T23:49:51.881351Z","iopub.execute_input":"2023-12-06T23:49:51.881966Z","iopub.status.idle":"2023-12-06T23:50:29.471907Z","shell.execute_reply.started":"2023-12-06T23:49:51.881928Z","shell.execute_reply":"2023-12-06T23:50:29.470447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_dates = pd.date_range(start='2022-11-08', end='2022-12-31')\nprediction_data = pd.DataFrame({\n    'Date': np.repeat(prediction_dates, len(le.classes_)),\n    'Id_compte_encoded': np.tile(le.transform(le.classes_), len(prediction_dates)),\n    'Category_encoded': np.tile(le.transform(le.classes_), len(prediction_dates)),\n    'Type_encoded': np.tile(le.transform(le.classes_), len(prediction_dates))\n})\n\n# Extracting month and day for prediction\nprediction_data = create_features(prediction_data)\n\n# Prepare features for prediction\nX_prediction = prediction_data[['dayofweek', 'quarter', 'month', 'year', 'dayofyear', 'dayofmonth', 'weekofyear', 'Category_encoded', 'Type_encoded', 'Id_compte_encoded']]\n\n# Make predictions for the evaluation period\npredicted_amounts = best_model.predict(X_prediction)\nprediction_data['Predicted_Amount'] = predicted_amounts\nprediction_data['ID'] = prediction_data['Date'].dt.strftime('%Y-%m-%d') + '_' + le.inverse_transform(prediction_data['Id_compte_encoded'])\n\n# Preparing the final output\nfinal_output = prediction_data[['ID', 'Predicted_Amount']]\n\n# Save the final output data to a CSV file\nfinal_output.to_csv('final_output.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}